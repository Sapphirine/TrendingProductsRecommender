{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'happybase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-07ea6a2f552b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjstyleson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhappybase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'happybase'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "import socket\n",
    "from urllib.parse import urlencode\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import jstyleson\n",
    "import happybase\n",
    "import getpass\n",
    "import itertools\n",
    "import struct\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from multiprocessing import Process\n",
    "\n",
    "from Memoized import Memoized\n",
    "\n",
    "\n",
    "@Memoized\n",
    "def get_oauth(consumer_key, consumer_secret, access_token, access_secret):\n",
    "    return requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_secret)\n",
    "\n",
    "\n",
    "def get_tweets(session, query_terms, oauth_token):\n",
    "    #url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    url = 'https://api.twitter.com/1.1/search/tweets.json'\n",
    "    query_params = urlencode({'language': 'en', 'track': ','.join(query_terms)}).replace('+','%20')\n",
    "    query_url = url + '?' + query_params\n",
    "    response = session.get(query_url, auth=oauth_token,\n",
    "                            stream=True)\n",
    "    logging.warning(\"%s: %s\", query_url, response)\n",
    "    return response\n",
    "\n",
    "\n",
    "def writeTweetToHbase(tweet, connection_pool, table):\n",
    "    user = {}\n",
    "    if 'user' in tweet:\n",
    "        for k,v in tweet['user'].items():\n",
    "            if isinstance(v, int):\n",
    "                user['user:{}'.format(k).encode('utf-8')] = struct.pack(\">Q\", v)\n",
    "            else:\n",
    "                user['user:{}'.format(k).encode('utf-8')] = str(v).encode('utf-8')\n",
    "    rowkey = tweet['id_str'].encode('utf-8')\n",
    "    for key in ['user', 'entities', 'extended_entities', 'id_str']:\n",
    "        if key in tweet:\n",
    "            del tweet[key]\n",
    "    enctweet = {}\n",
    "    for k,v in tweet.items():\n",
    "        if isinstance(v, int):\n",
    "            enctweet['tweet:{}'.format(k).encode('utf-8')] = struct.pack(\">Q\", v)\n",
    "        else:\n",
    "            enctweet['tweet:{}'.format(k).encode('utf-8')] = str(v).encode('utf-8')\n",
    "\n",
    "    with connection_pool.connection() as connection:\n",
    "        connection.open()\n",
    "        connection.table(table).put(\n",
    "            rowkey,\n",
    "            {**user, **enctweet}\n",
    "        )\n",
    "        connection.close()\n",
    "\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection, connection_pool, table, category):\n",
    "    for line in http_resp.iter_lines():\n",
    "        if len(line) > 0:\n",
    "            try:\n",
    "                full_tweet = json.loads(line.decode('utf-8'))\n",
    "                full_tweet['category'] = category\n",
    "                writeTweetToHbase(full_tweet, connection_pool, table)\n",
    "                # print(full_tweet)\n",
    "                tweet_text = full_tweet['text']\n",
    "                # print(tweet_text)\n",
    "                tcp_connection.send((tweet_text + '\\n').encode())\n",
    "            except Exception as exc:\n",
    "                e = sys.exc_info()[0]\n",
    "                print(e)\n",
    "\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        print(\"----------- %s -----------\" % str(time))\n",
    "        hashtag_counts_df = None\n",
    "        try:\n",
    "            # Get spark sql singleton context from the current context\n",
    "            sql_context = get_sql_context_instance(rdd.context)\n",
    "            # convert the RDD to Row RDD\n",
    "            row_rdd = rdd.map(lambda w: Row(term=w[0], term_count=w[1]))\n",
    "            # create a DF from the Row RDD\n",
    "            if not row_rdd.isEmpty():\n",
    "                hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "                # Register the dataframe as table\n",
    "                hashtags_df.registerTempTable(\"search_terms\")\n",
    "                # get the top 10 hashtags from the table using SQL and print them\n",
    "                all_terms = []\n",
    "                for v in query_terms.values():\n",
    "                    all_terms += v\n",
    "                hashtag_counts_df = sql_context.sql(\n",
    "                    \"select `term`, `term_count` from `search_terms` WHERE `term` IN {0}\".format(\n",
    "                        tuple(all_terms)\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            try:\n",
    "                os.system('cls' if os.name == 'nt' else 'clear')\n",
    "                if hashtag_counts_df is not None:\n",
    "                   hashtag_counts_df\\\n",
    "                        .orderBy('term_count', ascending=False)\\\n",
    "                        .show(10)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n",
    "def sum_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def stream_to_spark(name, host, port):\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(name)\n",
    "    # create spark context with the above configuration\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    # create the Streaming Context from the above spark context with interval\n",
    "    # size 1 seconds\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "    # setting a checkpoint to allow RDD recovery\n",
    "    ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "    # read data from port 9009\n",
    "    dataStream = ssc.socketTextStream(host, port)\n",
    "    # dataStream.pprint()\n",
    "    # split each tweet into words\n",
    "    words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "    # filter the words to get only hashtags, then map each hashtag to\n",
    "    # # be a pair of (hashtag,1)\n",
    "    # words = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "    words = words.map(lambda x: (x, 1))\n",
    "    # # adding the count of each hashtag to its last count\n",
    "    # tags_totals = hashtags.updateStateByKey(sum_tags_count)\n",
    "    tags_totals = words.updateStateByKey(sum_tags_count)\n",
    "    # do processing for each RDD generated in each interval\n",
    "    tags_totals.foreachRDD(process_rdd)\n",
    "    # start the streaming computation\n",
    "    ssc.start()\n",
    "    # wait for the streaming to finish\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-32019ff640d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m                         default='0.92')\n\u001b[1;32m    100\u001b[0m     parser.add_argument('--hbase_table_prefix', help='hbase table prefix / namespace', type=str,\n\u001b[0;32m--> 101\u001b[0;31m                         default='{}_{}'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"), getpass.getuser()))\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mloglevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "HBASE_SCHEMA = {\n",
    "    'tweet': dict(),\n",
    "    'user': dict()\n",
    "}\n",
    "\n",
    "def get_phrases(query_terms):\n",
    "    phrases = {}\n",
    "    for category in [key for key in query_terms.keys() if key != 'general']:\n",
    "        phrases[category] = [' '.join(pair) for pair in itertools.product(query_terms['general'], query_terms[category])]\n",
    "    return phrases\n",
    "\n",
    "def main(\n",
    "        query_terms,\n",
    "        consumer_key,\n",
    "        consumer_secret,\n",
    "        access_token,\n",
    "        access_secret,\n",
    "        spark_host,\n",
    "        spark_port,\n",
    "        spark_app_name,\n",
    "        num_ports,\n",
    "        hbase_table,\n",
    "        hbase_connection_pool_size,\n",
    "        hbase_host,\n",
    "        hbase_port,\n",
    "        hbase_thrift_version,\n",
    "        hbase_table_prefix\n",
    "    ):\n",
    "    conn = None\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    for target_port in range(spark_port, spark_port+num_ports):\n",
    "        try:\n",
    "            s.bind((spark_host, target_port))\n",
    "            logging.warning('Bound socket to %s:%d, listening', spark_host, target_port)\n",
    "            break\n",
    "        except OSError:\n",
    "            logging.warning('Port %d unavailable, trying next port', target_port)\n",
    "            if target_port == spark_port+num_ports-1:\n",
    "                logging.error('No ports available! Exiting.')\n",
    "                exit(1)\n",
    "    s.listen(1)\n",
    "    print(\"Waiting for TCP connection...\")\n",
    "    hbase_connection_pool = happybase.ConnectionPool(\n",
    "        hbase_connection_pool_size,\n",
    "        host=hbase_host,\n",
    "        port=hbase_port,\n",
    "        compat=hbase_thrift_version,\n",
    "        table_prefix=hbase_table_prefix)\n",
    "    with hbase_connection_pool.connection() as connection:\n",
    "        connection.open()\n",
    "        if str.encode(hbase_table) not in connection.tables():\n",
    "            connection.create_table(hbase_table, HBASE_SCHEMA)\n",
    "        connection.close()\n",
    "\n",
    "    spark_process = Process(target=stream_to_spark, args=(spark_app_name,spark_host,target_port))\n",
    "    spark_process.start()\n",
    "    conn, addr = s.accept()\n",
    "    print(\"Connected... Starting getting tweets.\")\n",
    "    processes = []\n",
    "    session = requests.session()\n",
    "    oauth_token = get_oauth(consumer_key, consumer_secret, access_token, access_secret)\n",
    "    phrases = get_phrases(query_terms)\n",
    "    for category in phrases:\n",
    "        if phrases[category]:\n",
    "            resp = get_tweets(session, phrases[category], oauth_token)\n",
    "            print(resp)\n",
    "            p = Process(target=send_tweets_to_spark, args=(resp, conn, hbase_connection_pool, hbase_table, category))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "    retvals = [p.join() for p in processes]\n",
    "    spark_process.join()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Twitter streamer parameters.')\n",
    "    parser.add_argument('-f', '--hashtags_file', help='A JSON file of hashtags category: [hashtags] pairs.', type=open,\n",
    "                        default='terms.json')\n",
    "    parser.add_argument('-l', '--loglevel', help='One of: DEBUG, INFO, WARNING, ERROR, CRITICAL', type=str,\n",
    "                        default='WARNING')\n",
    "    parser.add_argument('--logfile', help='Filename to write to. If not specified, write logs to stderr', type=str,\n",
    "                        default=None)\n",
    "    parser.add_argument('--spark_host', help='Host for spark server', type=str,\n",
    "                        default='localhost')\n",
    "    parser.add_argument('--spark_port', help='Port for spark server', type=int,\n",
    "                        default=9009)\n",
    "    parser.add_argument('--spark_app_name', help='Spark AppName for spark server. ', type=str,\n",
    "                        default='TwitterStreamApp')\n",
    "    parser.add_argument('--num_ports', help='Number of ports to try before giving up', type=int,\n",
    "                        default=100)\n",
    "    parser.add_argument('--hbase_table', help='Name of HBase table to which tweets will be written', type=str,\n",
    "                        default='tweets')\n",
    "    parser.add_argument('--hbase_connection_pool_size', help='HBase connection pool size', type=int,\n",
    "                        default=10)\n",
    "    parser.add_argument('--hbase_host', help='hbase host', type=str,\n",
    "                        default='localhost')\n",
    "    parser.add_argument('--hbase_port', help='hbase port', type=int,\n",
    "                        default=9090)\n",
    "    parser.add_argument('--hbase_thrift_version', help='hbase thrift version', type=str,\n",
    "                        default='0.92')\n",
    "    parser.add_argument('--hbase_table_prefix', help='hbase table prefix / namespace', type=str,\n",
    "                        default='{}_{}'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"), getpass.getuser()))\n",
    "    args = parser.parse_args()\n",
    "    loglevel = args.loglevel.upper()\n",
    "    if args.logfile is None:\n",
    "        logging.basicConfig(datefmt='%Y-%m-%d %H:%M:%S', format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                            level=loglevel)\n",
    "    else:\n",
    "        logging.basicConfig(datefmt='%Y-%m-%d %H:%M:%S', format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                            filename=args.logfile, level=loglevel)\n",
    "    query_terms = None\n",
    "    with args.hashtags_file as hf:\n",
    "        query_terms = jstyleson.load(hf)\n",
    "    logging.info(args)\n",
    "    logging.info(query_terms)\n",
    "    # Twitter credentials are read from env vars\n",
    "    consumer_key, consumer_secret, access_token, access_secret = os.environ['TWITTER_CONSUMER_KEY'],\\\n",
    "                                                                 os.environ['TWITTER_CONSUMER_SECRET'],\\\n",
    "                                                                 os.environ['TWITTER_ACCESS_TOKEN'],\\\n",
    "                                                                 os.environ['TWITTER_ACCESS_SECRET']\n",
    "    spark_host, spark_port, spark_app_name = args.spark_host, args.spark_port, args.spark_app_name\n",
    "    hbase_table_prefix = None if str.lower(args.hbase_table_prefix) == 'none' else args.hbase_table_prefix\n",
    "    main(\n",
    "        query_terms,\n",
    "        consumer_key,\n",
    "        consumer_secret,\n",
    "        access_token,\n",
    "        access_secret,\n",
    "        spark_host,\n",
    "        spark_port,\n",
    "        spark_app_name,\n",
    "        args.num_ports,\n",
    "        args.hbase_table,\n",
    "        args.hbase_connection_pool_size,\n",
    "        args.hbase_host,\n",
    "        args.hbase_port,\n",
    "        args.hbase_thrift_version,\n",
    "        hbase_table_prefix\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
